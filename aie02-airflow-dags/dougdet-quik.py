# Airflow
import os
import getpass
import logging
import shutil
from datetime import datetime, timedelta
from os import path
from airflow import DAG
from airflow.decorators import task
from airflow.models import Variable
from airflow.models.param import Param
from airflow.utils.dates import days_ago
from airflow.operators.python import get_current_context
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.weaviate.hooks.weaviate import WeaviateHook

auth_bkends = Variable.get("AIRFLOW__API__AUTH_BACKENDS")

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email': ['doug.parrish@hpe.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 0
}
today = datetime.today().strftime('%Y-%m-%d')
with DAG(
    'dougdet-quik',
    default_args=default_args,
    schedule_interval=None,
    tags=['test', 'dougdet'],
    params={
        'av_conn_id': Param("ce-dougdet-avdata", type="string"),
        'weave_conn_id': Param("ce-dougdet-weaviate", type="string"),
        's3_bucket': Param("ce-dougdet-pcaiexer", type="string"),
        's3_prefix': Param(f"documents/", type="string"),
        'shared_vol_base': Param(f"/mnt/shared/", type="string"),
        'dnld_path': Param("dougdet", type="string"),
        'dnld_dir': Param("from_airflow", type="string")
    },
    access_control={
        'All': {
            'can_read',
            'can_edit',
            'can_delete'
        }
    }
) as dag:

    @task
    def cleanup_export_dir():
        context = get_current_context()
        shared_vol_base = context['params']['shared_vol_base']
        dnld_path = context['params']['dnld_path']
        dnld_dir = context['params']['dnld_dir']
        export_dir = path.join(shared_vol_base, dnld_path, dnld_dir)
        logger = logging.getLogger(__name__)
        logger.info('auth_bkends = "' + auth_bkends + '"')
        logger.info('Environment variables:')
        for (k, v) in os.environ.items():
            logger.info(''.join(("\t", k, '="', v, '"')))
        if path.exists(export_dir):
            shutil.rmtree(export_dir)
            return f"Deleted directory {export_path}"
        return f"{export_dir} doesn't exist so nothing to delete"

    @task
    def get_all_filepaths_from_s3_path():
        context = get_current_context()
        av_conn_id = context['params']['av_conn_id']
        s3_bucket = context['params']['s3_bucket']
        s3_prefix = context['params']['s3_prefix']
        s3=S3Hook(aws_conn_id=av_conn_id)
        filepaths = s3.list_keys(bucket_name=s3_bucket, prefix=s3_prefix)
        return filepaths

    @task
    def download_s3_file_to_shared_volume(s3path):
        context = get_current_context()
        #
        # S3 identifiers
        av_conn_id = context['params']['av_conn_id']
        s3_bucket = context['params']['s3_bucket']
        s3_prefix = context['params']['s3_prefix']
        #
        # download dest identifiers
        shared_vol_base = context['params']['shared_vol_base']
        dnld_path = context['params']['dnld_path']
        dnld_dir = context['params']['dnld_dir']
        #
        # Make sure all subdirectories have been created before download
        subdir_in_s3 = path.dirname(s3path)
        s3_full_pfx = path.dirname(s3path)
        s3_sub_pfx = s3_full_pfx[(s3_full_pfx.find(s3_prefix)+len(s3_prefix)):]
        export_dir = path.join(shared_vol_base, dnld_path, dnld_dir, s3_sub_pfx)
        os.makedirs(export_dir, mode=0o775, exist_ok=True)
        os.umask(0o002)
        #
        # DOWNLOAD
        path_in_shared_volume = S3Hook(aws_conn_id=av_conn_id).download_file(
            s3_bucket=s3_bucket,
            key=s3path,
            local_path=export_dir,
            preserve_file_name=True,
            use_autogenerated_subdir=False
        )
        os.chmod(path_in_shared_volume, 0o664)
        return path_in_shared_volume

    cleanup_export_dir_task = cleanup_export_dir()

    cleanup_export_dir_task >> download_s3_file_to_shared_volume.expand(s3path=get_all_filepaths_from_s3_path())
