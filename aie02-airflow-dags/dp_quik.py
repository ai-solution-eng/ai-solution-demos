# Airflow
import os
import getpass
import shutil
from datetime import datetime, timedelta
from os import path
from airflow import DAG
from airflow.decorators import task
from airflow.models.param import Param
from airflow.utils.dates import days_ago
from airflow.operators.python import get_current_context
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.weaviate.hooks.weaviate import WeaviateHook

token_file = '/etc/secrets/ezua/.auth_token'
if path.exists(token_file):
    with open(token_file, 'r') as fd:
        auth_token = fd.read().strip()
        os.environ['AUTH_TOKEN'] = auth_token

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email': ['doug.parrish@hpe.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 0
}
today = datetime.today().strftime('%Y-%m-%d')
with DAG(
    'dougdet-quik',
    default_args=default_args,
    schedule_interval=None,
    tags=['test', 'dougdet'],
    params={
        'av_conn_id': Param("ce-dougdet-avdata", type="string"),
        'weave_conn_id': Param("ce-dougdet-weaviate", type="string"),
        's3_bucket': Param("ce-dougdet-pcaiexer", type="string"),
        's3_prefix': Param(f"documents/", type="string"),
        'shared_vol_base': Param(f"/mnt/shared/", type="string"),
        'dnld_path': Param("dougdet", type="string"),
        'dnld_dir': Param("from_airflow", type="string")
    },
    access_control={
        'All': {
            'can_read',
            'can_edit',
            'can_delete'
        }
    }
) as dag:

    @task
    def check_for_auth_token():
        if 'AUTH_TOKEN' in os.environ:
            print(f"AUTH_TOKEN={os.environ['AUTH_TOKEN']}")
        else:
            raise ValueError('AUTH_TOKEN not set!')
        return

    @task
    def cleanup_export_dir():
        context = get_current_context()
        shared_vol_base = context['params']['shared_vol_base']
        dnld_path = context['params']['dnld_path']
        dnld_dir = context['params']['dnld_dir']
        if 'AUTH_TOKEN' in os.environ:
            print(f"AUTH_TOKEN={os.environ['AUTH_TOKEN']}")
        else:
            raise ValueError('AUTH_TOKEN not set!')
        export_dir = path.join(shared_vol_base, dnld_path, dnld_dir)
        if path.exists(export_dir):
            shutil.rmtree(export_dir)
            return f"Deleted directory {export_path}"
        return f"{export_dir} doesn't exist so nothing to delete"
        
    @task
    def get_all_filepaths_from_s3_path():
        context = get_current_context()
        if 'AUTH_TOKEN' in os.environ:
            print(f"AUTH_TOKEN={os.environ['AUTH_TOKEN']}")
        else:
            raise ValueError('AUTH_TOKEN not set!')
        av_conn_id = context['params']['av_conn_id']
        s3_bucket = context['params']['s3_bucket']
        s3_prefix = context['params']['s3_prefix']
        s3=S3Hook(aws_conn_id=av_conn_id)
        filepaths = s3.list_keys(bucket_name=s3_bucket, prefix=s3_prefix)
        return filepaths

    @task
    def download_s3_file_to_shared_volume(s3path):
        context = get_current_context()
        #
        # S3 identifiers
        if 'AUTH_TOKEN' in os.environ:
            print(f"AUTH_TOKEN={os.environ['AUTH_TOKEN']}")
        else:
            raise ValueError('AUTH_TOKEN not set!')
        av_conn_id = context['params']['av_conn_id']
        s3_bucket = context['params']['s3_bucket']
        s3_prefix = context['params']['s3_prefix']
        #
        # download dest identifiers
        shared_vol_base = context['params']['shared_vol_base']
        dnld_path = context['params']['dnld_path']
        dnld_dir = context['params']['dnld_dir']
        #
        # Make sure all subdirectories have been created before download
        s3_full_pfx = path.dirname(s3path)
        s3_sub_pfx = s3_full_pfx[(s3_full_pfx.find(s3_prefix)+len(s3_prefix)):]
        export_dir = path.join(shared_vol_base, dnld_path, dnld_dir, s3_sub_pfx)
        os.makedirs(export_dir, mode=0o775, exist_ok=True)
        os.umask(0o002)
        #
        # DOWNLOAD
        path_in_shared_volume = S3Hook(aws_conn_id=av_conn_id).download_file(
            s3_bucket=s3_bucket,
            key=s3path,
            local_path=export_dir,
            preserve_file_name=True,
            use_autogenerated_subdir=False
        )
        os.chmod(path_in_shared_volume, 0o664)
        return path_in_shared_volume

    check_for_auth_token_task = check_for_auth_token()
    cleanup_export_dir_task = cleanup_export_dir()

    check_for_auth_token_task >> cleanup_export_dir_task >> download_s3_file_to_shared_volume.expand(s3path=get_all_filepaths_from_s3_path())
