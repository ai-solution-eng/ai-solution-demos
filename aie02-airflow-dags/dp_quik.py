# Airflow
import os
import getpass
import logging
import shutil
from datetime import datetime, timedelta
from os import path
from airflow import DAG
from airflow.decorators import task
from airflow.models.param import Param
from airflow.utils.dates import days_ago
from airflow.utils.log.logging_mixin import LoggingMixin
from airflow.operators.python import get_current_context
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
#from airflow.providers.weaviate.hooks.weaviate import WeaviateHook

token_file = '/etc/secrets/ezua/.auth_token'
if path.exists(token_file):
    with open(token_file, 'r') as fd:
        auth_token = fd.read().strip()
        os.environ['AUTH_TOKEN'] = auth_token

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email': ['doug.parrish@hpe.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 0
}
today = datetime.today().strftime('%Y-%m-%d')
with DAG(
    'dougdet-quik',
    default_args=default_args,
    schedule_interval=None,
    tags=['test', 'dougdet'],
    params={
        'av_conn_id': Param("ce-dougdet-avdata", type="string"),
        'weave_conn_id': Param("ce-dougdet-weaviate", type="string"),
        's3_bucket': Param("ce-dougdet-pcaiexer", type="string"),
        's3_prefix': Param(None, type="string"),
        'shared_vol_base': Param("/mnt/shared/", type="string"),
        'dnld_path': Param("dougdet", type="string"),
        'dnld_dir': Param("from_airflow", type="string")
    },
    access_control={
        'All': {
            'can_read',
            'can_edit',
            'can_delete'
        }
    }
) as dag:

    @task
    def display_envvars():
        for k, v in sorted(os.environ.items()):
            LoggingMixin().log.debug(f"{k}={v}")

    @task
    def cleanup_export_dir():
        context = get_current_context()
        shared_vol_base = context['params']['shared_vol_base']
        dnld_path = context['params']['dnld_path']
        dnld_dir = context['params']['dnld_dir']
        if 'AUTH_TOKEN' in os.environ:
            LoggingMixin().log.info(f"AUTH_TOKEN={os.environ['AUTH_TOKEN']}")
        else:
            LoggingMixin().log.info('AUTH_TOKEN not set')
        export_dir = path.join(shared_vol_base, dnld_path, dnld_dir)
        if path.exists(export_dir):
            shutil.rmtree(export_dir)
            LoggingMixin().log.info(f"Deleted directory {export_dir}.")
            msg = 'recreated'
        else:
            LoggingMixin().log.info(f"{export_dir} doesn't exist so nothing to delete")
            msg = 'created'
        os.mkdir(export_dir, mode=0o775)
        LoggingMixin().log.info(f"{export_dir} {msg}.")

    
    @task
    def download_s3_file_to_shared_volume():
        context = get_current_context()
        if 'AUTH_TOKEN' in os.environ:
            LoggingMixin().log.info(f"AUTH_TOKEN={os.environ['AUTH_TOKEN']}")
        else:
            LoggingMixin().log.info('AUTH_TOKEN not set')
        #
        # S3 identifiers
        av_conn_id = context['params']['av_conn_id']
        s3_bucket = context['params']['s3_bucket']
        s3_prefix = context['params']['s3_prefix']
        #
        # download dest identifiers
        shared_vol_base = context['params']['shared_vol_base']
        dnld_path = context['params']['dnld_path']
        dnld_dir = context['params']['dnld_dir']
        #
        # DOWNLOAD
        os.umask(0o022)
        s3=S3Hook(aws_conn_id=av_conn_id)
        for s3path in s3.list_keys(bucket_name=s3_bucket, prefix=s3_prefix):
            if s3path[-1] == '/':  # skip directories
                download_dir = path.join(shared_vol_base, dnld_path, dnld_dir, path.dirname(s3path))
                LoggingMixin().log.info(f"Creating {download_dir}")
                os.makedirs(download_dir, mode=0o755, exist_ok=True)
                continue
            LoggingMixin().log.info(f"Downloading {s3path}")
            #
            # PERFORM Download
            path_in_shared_volume = s3.download_file(
                bucket_name=s3_bucket,
                key=s3path,
                local_path=download_dir,
                preserve_file_name=True,
                use_autogenerated_subdir=False
            )
            os.chmod(path_in_shared_volume, 0o644)
            LoggingMixin().log.info(f"Downloaded {path_in_shared_volume}")
        
    display_envvars_task = display_envvars()
    cleanup_export_dir_task = cleanup_export_dir()
    #get_all_filepaths_from_s3_path_task = get_all_filepaths_from_s3_path()
    download_s3_file_to_shared_volume_task = download_s3_file_to_shared_volume()
    
    display_envvars_task >> cleanup_export_dir_task >> download_s3_file_to_shared_volume_task
#    cleanup_export_dir_task >> download_s3_file_to_shared_volume.expand(s3path=get_all_filepaths_from_s3_path())
