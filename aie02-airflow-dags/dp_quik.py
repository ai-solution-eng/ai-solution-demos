# Airflow
import os
import getpass
import shutil
from datetime import datetime, timedelta
from os import path
from airflow import DAG
from airflow.decorators import task
from airflow.models.param import Param
from airflow.utils.dates import days_ago
from airflow.operators.python import get_current_context
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.weaviate.hooks.weaviate import WeaviateHook

token_file = '/etc/secrets/ezua/.auth_token'
if path.exists(token_file):
    with open(token_file, 'r') as fd:
        auth_token = fd.read().strip()
        os.environ['AUTH_TOKEN'] = auth_token

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email': ['doug.parrish@hpe.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 0
}
today = datetime.today().strftime('%Y-%m-%d')
with DAG(
    'dougdet-quik',
    default_args=default_args,
    schedule_interval=None,
    tags=['test', 'dougdet'],
    params={
        'av_conn_id': Param("ce-dougdet-avdata", type="string"),
        'weave_conn_id': Param("ce-dougdet-weaviate", type="string"),
        's3_bucket': Param("ce-dougdet-pcaiexer", type="string"),
        's3_prefix': Param(f"documents/", type="string"),
        'shared_vol_base': Param(f"/mnt/shared/", type="string"),
        'dnld_path': Param("dougdet", type="string"),
        'dnld_dir': Param("from_airflow", type="string")
    },
    access_control={
        'All': {
            'can_read',
            'can_edit',
            'can_delete'
        }
    }
) as dag:

    @task
    def display_envvars():
        for k, v in sorted(os.environ.items()):
            print(f"{k}={v}")

    @task
    def cleanup_export_dir():
        context = get_current_context()
        shared_vol_base = context['params']['shared_vol_base']
        dnld_path = context['params']['dnld_path']
        dnld_dir = context['params']['dnld_dir']
        if 'AUTH_TOKEN' in os.environ:
            print(f"AUTH_TOKEN={os.environ['AUTH_TOKEN']}")
        else:
            print('AUTH_TOKEN not set')
        export_dir = path.join(shared_vol_base, dnld_path, dnld_dir)
        if path.exists(export_dir):
            shutil.rmtree(export_dir)
            return f"Deleted directory {export_dir}"
        return f"{export_dir} doesn't exist so nothing to delete"
        
#    @task
#    def get_all_filepaths_from_s3_path():
#        context = get_current_context()
#        if 'AUTH_TOKEN' in os.environ:
#            print(f"AUTH_TOKEN={os.environ['AUTH_TOKEN']}")
#        else:
#            print('AUTH_TOKEN not set')
#        av_conn_id = context['params']['av_conn_id']
#        s3_bucket = context['params']['s3_bucket']
#        s3_prefix = context['params']['s3_prefix']
#        s3=S3Hook(aws_conn_id=av_conn_id)
#        filepaths = s3.list_keys(bucket_name=s3_bucket, prefix=s3_prefix)
#        return filepaths

    @task
    def download_s3_file_to_shared_volume():
        context = get_current_context()
        if 'AUTH_TOKEN' in os.environ:
            print(f"AUTH_TOKEN={os.environ['AUTH_TOKEN']}")
        else:
            print('AUTH_TOKEN not set')
        #
        # S3 identifiers
        av_conn_id = context['params']['av_conn_id']
        s3_bucket = context['params']['s3_bucket']
        s3_prefix = context['params']['s3_prefix']
        #
        # download dest identifiers
        shared_vol_base = context['params']['shared_vol_base']
        dnld_path = context['params']['dnld_path']
        dnld_dir = context['params']['dnld_dir']
        #
        # DOWNLOAD
        s3=S3Hook(aws_conn_id=av_conn_id)
        for s3path in s3.list_keys(bucket_name=s3_bucket, prefix=s3_prefix):
            #
            # Make sure all subdirectories have been created before download
            s3_full_pfx = path.dirname(s3path)
            s3_sub_pfx = s3_full_pfx[(s3_full_pfx.find(s3_prefix)+len(s3_prefix)):]
            export_dir = path.join(shared_vol_base, dnld_path, dnld_dir, s3_sub_pfx)
            os.makedirs(export_dir, mode=0o775, exist_ok=True)
            os.umask(0o022)
            #
            # PERFORM Download
            print(f"Downloading {s3path}")
            path_in_shared_volume = s3.download_file(
                bucket_name=s3_bucket,
                key=s3path,
                local_path=export_dir,
                preserve_file_name=True,
                use_autogenerated_subdir=False
            )
            os.chmod(path_in_shared_volume, 0o644)
            print(f"Downloaded {path_in_shared_volume}")
        
    display_envvars_task = display_envvars()
    cleanup_export_dir_task = cleanup_export_dir()
    #get_all_filepaths_from_s3_path_task = get_all_filepaths_from_s3_path()
    download_s3_file_to_shared_volume_task = download_s3_file_to_shared_volume()
    
    display_envvars_task >> cleanup_export_dir_task >> download_s3_file_to_shared_volume_task
#    cleanup_export_dir_task >> download_s3_file_to_shared_volume.expand(s3path=get_all_filepaths_from_s3_path())
