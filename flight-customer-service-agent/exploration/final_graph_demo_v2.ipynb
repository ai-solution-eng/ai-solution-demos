{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain.schema import Document  # Optional: For using Document objects\n",
    "from langchain_core.runnables import chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever(docs_list, embeddings):\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=250, chunk_overlap=0\n",
    "    )\n",
    "\n",
    "    # (Optional) Convert dictionaries to Document objects for better compatibility\n",
    "    documents = [Document(page_content=doc['page_content'], metadata=doc['metadata']) for doc in docs_list]\n",
    "\n",
    "    # Split the documents into smaller chunks\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Add to vectorDB\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=split_docs,\n",
    "        collection_name=\"sql-rag-test3\",\n",
    "        embedding=embeddings,\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={'k': 3})\n",
    "    return retriever\n",
    "\n",
    "def call_db(local_db_path=None, query=None, info=None):\n",
    "    assert local_db_path is not None\n",
    "    assert query is not None\n",
    "    assert info is not None\n",
    "    with sqlite3.connect(local_db_path) as local_conn:\n",
    "        cursor = local_conn.cursor()\n",
    "        cursor.execute(query, (info,))\n",
    "        rows = cursor.fetchall()\n",
    "        column_names = [column[0] for column in cursor.description]\n",
    "        results = [dict(zip(column_names, row)) for row in rows]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormatAns(BaseModel):\n",
    "    \"\"\"Structured output for generated SQL query.\"\"\"\n",
    "    user_input: str = Field(description='The user specified input for the SQL query.')\n",
    "    sql_query: str = Field(description=\"Syntactically valid SQL query.\")\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future.\"\"\"\n",
    "    plan: str = Field(description=\"Steps to follow, should be in sorted order\")\n",
    "\n",
    "# Data model - how to track information over states\n",
    "class Grade(BaseModel):\n",
    "    \"\"\"Binary score for relevance check to generate sql or do RAG.\"\"\"\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original sql generation prompt\n",
    "template = \"\"\"\n",
    "You are an assistant for generating sql queries. Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. Only answer with a sql query.\n",
    "**When the sql is generated, replace any user defined values with a question mark.**\n",
    "===\n",
    "Example:\n",
    "Question: What airport am I flying out of? my passenger id is \\\"3442 587242.\\\"\n",
    "Context:[Document(page_content='How to answer questions with What. Database has 11 rows: seat_no, fare_conditions, scheduled_arrival, scheduled_departure, ticket_no, book_ref, passenger_id, flight_id, flight_no, departure_airport, arrival_airport.\\n        The database is named query_results.')]\n",
    "user_input: 3442 587242\n",
    "sql_query: SELECT departure_airport FROM query_results WHERE passenger_id = ?\n",
    "===\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "\"\"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Updated process prompt now includes conversation history\n",
    "process_template = \"\"\"\n",
    "You are an assistant that answers a user's question based on json. \n",
    "Here is the conversation history so far:\n",
    "{history}\n",
    "\n",
    "Answer the question using the following retrieved context:\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "If you don't know the answer, just say that you don't know. Answer in a personal, conversational tone.\n",
    "\"\"\"\n",
    "\n",
    "process_prompt = ChatPromptTemplate.from_template(process_template)\n",
    "\n",
    "# Add prompt to create a plan on how to generate sql query\n",
    "plan_template = \"\"\"\n",
    "You are a helpful assistant that generates two precise plans on how to implement a sql query.\n",
    "Make sure to pay attention to the user input provided in the question.\n",
    "===\n",
    "Question: What airport am I flying out of? my passenger id is \\\"3442 587242.\\\"\n",
    "Context:[Document(page_content='How to answer questions with What. Database has 11 rows: seat_no, fare_conditions, scheduled_arrival, scheduled_departure, ticket_no, book_ref, passenger_id, flight_id, flight_no, departure_airport, arrival_airport.\\n        The database is named query_results.')]\n",
    "Plan: This SQL command retrieves all columns (SELECT *) from a table named query_results where the passenger_id column matches the value provided by the ? placeholder. The ? is a parameter marker, indicating a value will be passed in later to complete the query.\n",
    "===\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "\"\"\"\n",
    "\n",
    "plan_prompt = ChatPromptTemplate.from_template(plan_template)\n",
    "\n",
    "# Add prompt to create plan on executing plan\n",
    "exec_template = \"\"\"\n",
    "You are an assistant for generating sql queries. Use the following pieces of a plan to answer the question. \n",
    "If you don't know the answer, just say that you don't know. Only answer with a sql query. When the sql is generated, replace the user defined values with a question mark.\n",
    "*Do not have user input be in brackets or quotations at the final answer.\n",
    "*Make sure to pay attention to the user input value, [ticket_no] is not correct.\n",
    "Question: {question} \n",
    "Plan: {plan} \n",
    "\"\"\"\n",
    "execute_prompt = ChatPromptTemplate.from_template(exec_template)\n",
    "\n",
    "process_template2 = \"\"\"\n",
    "You are an assistant that answers a user's question based on json. Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. Do not answer based on the plan. Answer in a personal, conversational tone.\n",
    "Question: {question} \n",
    "Plan: {plan}\n",
    "Context: {context} \n",
    "\"\"\"\n",
    "\n",
    "process_prompt2 = ChatPromptTemplate.from_template(process_template2)\n",
    "\n",
    "# Add prompt to create plan on how to generate sql query\n",
    "grade_template = \"\"\"\n",
    "Give a binary score 'yes' or 'no' to indicate whether the context and the question needs sql query generation.\n",
    "Make sure to pay attention to the user input provided in the question.\n",
    "===\n",
    "Examples:\n",
    "Question: How can I reschedule my flight?\n",
    "Context: How to reschedule a flight: need to email returns@hpe.com, submit request, and someone will get back to you.\n",
    "binary_score=no\n",
    "\n",
    "Question: Can you tell me about my flight? my passenger id is \\\"3442 587242.\\\"\n",
    "Context: [Document( page_content='How to answer questions with What. Database has 11 rows: seat_no, fare_conditions, scheduled_arrival, scheduled_departure, ticket_no, book_ref, passenger_id, flight_id, flight_no, departure_airport, arrival_airport.\\n        The database is named query_results.'), Document( page_content='How to reschedule a flight: need to email returns@hpe.com, submit request, and someone will get back to you.'), Document( page_content='How to submit expense report: go to concur through home.hpe.com, and follow the necessary forms.')] \n",
    "binary_score=yes\n",
    "===\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_template(grade_template)\n",
    "\n",
    "rag_template = \"\"\"\n",
    "You are a helpful assistant that answers question based on retrieved context.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initalize Vector DB and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: uncomment if you want to use \n",
    "\n",
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "#     nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "#     assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "#     os.environ[\"NVIDIA_API_KEY\"] = nvapi_key\n",
    "# os.environ[\"NVIDIA_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG DB\n",
    "docs_list = [\n",
    "    {'metadata': {}, 'page_content': \"\"\"\n",
    "    How to answer questions with What. Database has 11 rows: seat_no, fare_conditions, scheduled_arrival, scheduled_departure, ticket_no, book_ref, passenger_id, flight_id, flight_no, departure_airport, arrival_airport.\n",
    "    The database is named query_results. \n",
    "    \"\"\"},\n",
    "    {'metadata': {}, 'page_content': 'How to reschedule a flight: need to email returns@hpe.com, submit request, and someone will get back to you.'},\n",
    "    {'metadata': {}, 'page_content': 'How to submit expense report: go to concur through home.hpe.com, and follow the necessary forms.'}\n",
    "]\n",
    "\n",
    "# NOTE: uncomment if you want to use NVIDIA Embedding model\n",
    "# embeddings = NVIDIAEmbeddings(base_url='https://integrate.api.nvidia.com/v1',\n",
    "#                                model=\"nvidia/nv-embed-v1\", \n",
    "#                                api_key=os.environ[\"NVIDIA_API_KEY\"],\n",
    "#                                truncate=\"NONE\")\n",
    "\n",
    "embeddings = NVIDIAEmbeddings(base_url='http://embedding-tyler.models.mlds-kserve.us.rdlabs.hpecorp.net',\n",
    "                               model=\"thenlper/gte-base\", \n",
    "                               api_key='',\n",
    "                               truncate=\"NONE\")\n",
    "\n",
    "retriever = create_retriever(docs_list, embeddings)\n",
    "local_db_path = 'tickets_joined.db'\n",
    "\n",
    "# Create models\n",
    "llm = ChatNVIDIA(base_url=\"http://10.182.1.167:8080/v1\",\n",
    "                  model=\"meta/llama-3.1-70b-instruct\", \n",
    "                  api_key=\"\\'\\'\",\n",
    "                  verbose=True)\n",
    "formatted_llm = ChatNVIDIA(base_url=\"http://10.182.1.167:8080/v1\",\n",
    "                            model=\"meta/llama-3.1-70b-instruct\", \n",
    "                            api_key=\"\\'\\'\",\n",
    "                            verbose=True).with_structured_output(FormatAns)\n",
    "plan_llm = ChatNVIDIA(base_url=\"http://10.182.1.167:8080/v1\",\n",
    "                     model=\"meta/llama-3.1-70b-instruct\", \n",
    "                     api_key=\"\\'\\'\",\n",
    "                     verbose=True).with_structured_output(Plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities and code needed to run agent with LangGraph\n",
    "\n",
    "# Extend the GraphState to include a conversation history (list of messages)\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: The current user question.\n",
    "        history: List of messages (each is a dict with 'role' and 'content').\n",
    "        generation: LLM generation (assistant response).\n",
    "        structured_sql_query: Generated SQL query with user defined value.\n",
    "        sql_results: Data returned from SQL query (list of JSON dicts).\n",
    "        plan: LLM generated plan on how to generate the SQL query.\n",
    "        documents: List of documents.\n",
    "        sql_gen_check: Grade, a binary score ('yes' or 'no').\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    history: List[Dict[str, str]]\n",
    "    generation: str\n",
    "    structured_sql_query: FormatAns\n",
    "    sql_results: List[Dict]\n",
    "    plan: Plan\n",
    "    documents: List[str]\n",
    "    sql_gen_check: Grade\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    print(\"---RETRIEVE---\")\n",
    "    # Initialize history if not already present and add the user question\n",
    "    state.setdefault(\"history\", [])\n",
    "    state[\"history\"].append({\"role\": \"user\", \"content\": state[\"question\"]})\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"documents\": documents, \"question\": question, 'plan': None, \"history\": state[\"history\"]}\n",
    "\n",
    "\n",
    "def gen_sql_query(state):\n",
    "    print(\"---GENERATE_SQL_QUERY---\")\n",
    "    formatted = chat_prompt.invoke({\"context\": state['documents'], \n",
    "                                    \"question\": state['question']})\n",
    "    query_result = formatted_llm.invoke(formatted)\n",
    "    return {\"documents\": state['documents'], \n",
    "            \"question\": state['question'],\n",
    "            'plan': state['plan'],\n",
    "            \"structured_sql_query\": query_result}\n",
    "\n",
    "def execute_sql_query(state):\n",
    "    print(\"---EXEC_SQL_QUERY---\")\n",
    "    print(state)\n",
    "    query_result = state['structured_sql_query']\n",
    "    r = None\n",
    "    try:\n",
    "        r = call_db(local_db_path=local_db_path,\n",
    "                   query=query_result.sql_query,\n",
    "                   info=query_result.user_input)\n",
    "        print(\"r: \", r)\n",
    "        return {\"documents\": state['documents'], \n",
    "                \"question\": state['question'],\n",
    "                \"structured_sql_query\": state['structured_sql_query'],\n",
    "                'plan': state['plan'],\n",
    "                \"sql_results\": r}\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return {\"documents\": state['documents'], \n",
    "                \"question\": state['question'],\n",
    "                \"structured_sql_query\": state['structured_sql_query'],\n",
    "                'plan': state['plan'],\n",
    "                \"sql_results\": r}\n",
    "\n",
    "def answer(state):\n",
    "    print(\"---ANSWER---\")\n",
    "    print(\"state: \", state)\n",
    "    # Combine the conversation history into a string\n",
    "    conversation_history = \"\\n\".join(f\"{msg['role']}: {msg['content']}\" for msg in state[\"history\"])\n",
    "    \n",
    "    final_format = process_prompt.invoke({\n",
    "        \"history\": conversation_history,\n",
    "        \"context\": state[\"sql_results\"],\n",
    "        \"question\": state[\"question\"]\n",
    "    })\n",
    "    print(final_format.to_string())\n",
    "    answer_result = llm.invoke(final_format)\n",
    "    \n",
    "    # Append the assistant's answer to the conversation history\n",
    "    state[\"history\"].append({\"role\": \"assistant\", \"content\": answer_result.content})\n",
    "    \n",
    "    return {\"documents\": state[\"documents\"], \n",
    "            \"question\": state[\"question\"], \n",
    "            \"structured_sql_query\": state['structured_sql_query'],\n",
    "            \"sql_results\": state['sql_results'],\n",
    "            'plan': state['plan'],\n",
    "            \"generation\": answer_result,\n",
    "            \"history\": state[\"history\"]}\n",
    "\n",
    "def plan(state):\n",
    "    docs = state['documents']\n",
    "    question = state['question']\n",
    "    plan_result = plan_prompt.invoke({\"context\": docs, \"question\": question})\n",
    "    ans = plan_llm.invoke(plan_result)\n",
    "    return {'question': state['question'],\n",
    "            'documents': state['documents'],\n",
    "            'plan': ans}\n",
    "\n",
    "# update function\n",
    "def gen_sql_query2(state):\n",
    "    print(\"---GENERATE_SQL_QUERY---\")\n",
    "    plan_obj = state['plan']\n",
    "    docs = state['documents']\n",
    "    question = state['question']\n",
    "    exec_prompt = execute_prompt.invoke({\"plan\": plan_obj.plan, \"question\": question, 'context': docs})\n",
    "    print(exec_prompt.to_string())\n",
    "    query_result = llm.with_structured_output(FormatAns).invoke(exec_prompt)\n",
    "    print(\"query_result.user_input: \", query_result.user_input)\n",
    "    print(\"query_result.sql_query: \", query_result.sql_query)\n",
    "    return {\"documents\": state['documents'], \n",
    "            \"question\": state['question'],\n",
    "            'plan': state['plan'],\n",
    "            \"structured_sql_query\": query_result}\n",
    "\n",
    "def check(state):\n",
    "    docs = state['documents']\n",
    "    print(\"docs: \", docs)\n",
    "    question = state['question']\n",
    "    print(\"question: \", question)\n",
    "    grade = grade_prompt.invoke({\"context\": docs, \"question\": question})\n",
    "    ans = llm.with_structured_output(Grade).invoke(grade)\n",
    "    print(\"ans: \", ans)\n",
    "    return {\"documents\": state['documents'], \"question\": state['question'], 'sql_gen_check': ans, 'plan': None}\n",
    "\n",
    "def decide(state):\n",
    "    ans = state['sql_gen_check'].binary_score\n",
    "    if ans == 'yes':\n",
    "        return 'yes'\n",
    "    elif ans == 'no':\n",
    "        return 'no'\n",
    "\n",
    "def rag_answer(state):\n",
    "    print(\"---ANSWER---\")\n",
    "    print(\"state: \", state)\n",
    "    # Combine the conversation history into a string\n",
    "    conversation_history = \"\\n\".join(f\"{msg['role']}: {msg['content']}\" for msg in state[\"history\"])\n",
    "    \n",
    "    final_format = process_prompt.invoke({\n",
    "        \"history\": conversation_history,\n",
    "        \"context\": state['documents'],\n",
    "        \"question\": state[\"question\"]\n",
    "    })\n",
    "    print(final_format.to_string())\n",
    "    answer_result = llm.invoke(final_format)\n",
    "    \n",
    "    # Append the assistant's answer to the conversation history\n",
    "    state[\"history\"].append({\"role\": \"assistant\", \"content\": answer_result.content})\n",
    "    \n",
    "    return {\"documents\": state[\"documents\"], \n",
    "            \"question\": state[\"question\"], \n",
    "            'plan': state['plan'],\n",
    "            \"generation\": answer_result,\n",
    "            \"history\": state[\"history\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Build graph\n",
    "builder3 = StateGraph(GraphState)\n",
    "builder3.add_node(\"retrieve\", retrieve)\n",
    "builder3.add_node(\"check\", check)\n",
    "builder3.add_node(\"generate_sql_query\", gen_sql_query2)\n",
    "builder3.add_node(\"execute_sql_query\", execute_sql_query)\n",
    "builder3.add_node(\"planning\", plan)\n",
    "builder3.add_node(\"answer\", answer)\n",
    "builder3.add_node(\"rag_answer\", rag_answer)\n",
    "\n",
    "# Build Edges\n",
    "builder3.add_edge(START, \"retrieve\")\n",
    "builder3.add_edge('retrieve','check')\n",
    "builder3.add_conditional_edges('check',\n",
    "                               decide,\n",
    "                               {\n",
    "                                   'yes':'planning',\n",
    "                                   'no': 'rag_answer'\n",
    "                               }\n",
    "                              )\n",
    "builder3.add_edge(\"planning\", 'generate_sql_query')\n",
    "builder3.add_edge(\"generate_sql_query\", \"execute_sql_query\")\n",
    "builder3.add_edge(\"execute_sql_query\", \"answer\")\n",
    "builder3.add_edge(\"answer\", END)\n",
    "\n",
    "# Compile\n",
    "app3 = builder3.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to display the graph if needed\n",
    "# from IPython.display import Image, display\n",
    "# display(Image(app3.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run graph with conversation history support\n",
    "inputs = {\"question\": \"Can you tell me about my flight's departure time? my ticket_no is \\\"7240005432906569\\\".\",\n",
    "          'history':[]}\n",
    "for output in app3.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Node '{key}':\")\n",
    "print(value[\"generation\"].content)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run graph with conversation history support\n",
    "inputs = {\"question\": \"can you clarify the time in PM?\",\n",
    "          'history':[{'role': 'user', \n",
    "                      'content': 'Can you tell me about my flight\\'s departure time? my ticket_no is \"7240005432906569\".'}, \n",
    "                      {'role': 'assistant', 'content': 'Your flight is scheduled to depart on January 17th, 2025, at 16:35:25, so be sure to arrive at the airport with plenty of time to spare.'}]}\n",
    "for output in app3.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Node '{key}':\")\n",
    "print(value[\"generation\"].content)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run graph for a non-sql case\n",
    "inputs = {\"question\": \"How can I reschedule my flight?\",'history':[]}\n",
    "for output in app3.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Node '{key}':\")\n",
    "print(value[\"generation\"].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nim-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
