{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain.schema import Document  # Optional: For using Document objects\n",
    "from langchain_core.runnables import chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever(docs_list,embeddings):\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=250, chunk_overlap=0\n",
    "    )\n",
    "\n",
    "    # (Optional) Convert dictionaries to Document objects for better compatibility\n",
    "    documents = [Document(page_content=doc['page_content'], metadata=doc['metadata']) for doc in docs_list]\n",
    "\n",
    "    # Split the documents into smaller chunks\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "    # Add to vectorDB\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=split_docs,\n",
    "        collection_name=\"sql-rag-test3\",\n",
    "        embedding=embeddings,\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={'k': 3})\n",
    "    return retriever\n",
    "\n",
    "def call_db(local_db_path=None,query=None,info=None):\n",
    "    assert local_db_path is not None\n",
    "    assert query is not None\n",
    "    assert info is not None\n",
    "    # print(\"local_db_path: \",local_db_path)\n",
    "    local_conn = sqlite3.connect(local_db_path)\n",
    "    with sqlite3.connect(local_db_path) as local_conn:\n",
    "        cursor = local_conn.cursor()\n",
    "        cursor.execute(query, (info,))\n",
    "        rows = cursor.fetchall()\n",
    "        column_names = [column[0] for column in cursor.description]\n",
    "        results = [dict(zip(column_names, row)) for row in rows]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormatAns(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    user_input: str = Field(description='The user specified input for the SQL query.')\n",
    "    sql_query: str = Field(\n",
    "        description=\"Syntactically valid SQL query.\"\n",
    "    )\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    plan: str = Field(\n",
    "        description=\"steps to follow, should be in sorted order\"\n",
    "    )\n",
    "\n",
    "# Data model - how to track information over states\n",
    "class Grade(BaseModel):\n",
    "    \"\"\"Binary score for relevance check to generate sql or do rag.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "template = \"\"\"\n",
    "You are an assistant for generating sql queries. Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. Only answer with a sql query.\n",
    "**When the sql is generated, replace any user defined values with a question mark.**\n",
    "===\n",
    "Example:\n",
    "Question: What airport am I flying out of? my passenger id is \\\"3442 587242.\\\"\n",
    "Context:[Document(page_content='How to answer questions with What. Database has 11 rows: seat_no, fare_conditions, scheduled_arrival, scheduled_departure, ticket_no, book_ref, passenger_id, flight_id, flight_no, departure_airport, arrival_airport.\\n        The database is named query_results.')]\n",
    "user_input: 3442 587242\n",
    "sql_query: SELECT departure_airport FROM query_results WHERE passenger_id = ?\n",
    "===\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "\"\"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "process_template = \"\"\"\n",
    "You are an assistant that answers a user's question based on json. Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. Answer in a personal, conversational tone.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "\"\"\"\n",
    "\n",
    "process_prompt = ChatPromptTemplate.from_template(process_template)\n",
    "\n",
    "# Add prompt to creates plan on how to generate sql query\n",
    "plan_template = \"\"\"\n",
    "You are a helpful assistant that generates two precise plan on how to implement a sql query.\n",
    "Make sure to pay attention to the user input provided in the question.\n",
    "===\n",
    "Question: What airport am I flying out of? my passenger id is \\\"3442 587242.\\\"\n",
    "Context:[Document(page_content='How to answer questions with What. Database has 11 rows: seat_no, fare_conditions, scheduled_arrival, scheduled_departure, ticket_no, book_ref, passenger_id, flight_id, flight_no, departure_airport, arrival_airport.\\n        The database is named query_results.')]\n",
    "Plan: This SQL command retrieves all columns (SELECT *) from a table named query_results where the passenger_id column matches the value provided by the ? placeholder. The ? is a parameter marker, indicating a value will be passed in later to complete the query.\n",
    "===\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "plan_prompt = ChatPromptTemplate.from_template(plan_template)\n",
    "\n",
    "# Add prompt to creates plan on executing plan\n",
    "\n",
    "\n",
    "exec_template = \"\"\"\n",
    "You are an assistant for generating sql queries. Use the following pieces of a plan to answer the question. \n",
    "If you don't know the answer, just say that you don't know. Only answer with a sql query. When the sql is generated, replace the user defined values with a question mark.\n",
    "*Do not have user input be in brackets or quotations at the final answer.\n",
    "*Make sure to pay attention to the user input value, [ticket_no] is not correct.\n",
    "Question: {question} \n",
    "Plan: {plan} \n",
    "\"\"\"\n",
    "execute_prompt = ChatPromptTemplate.from_template(exec_template)\n",
    "\n",
    "process_template2 = \"\"\"\n",
    "You are an assistant that answers a user's question based on json. Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. Do not answer based on the plan. Answer in a personal, conversational tone.\n",
    "Question: {question} \n",
    "Plan: {plan}\n",
    "Context: {context} \n",
    "\"\"\"\n",
    "\n",
    "process_prompt2 = ChatPromptTemplate.from_template(process_template2)\n",
    "\n",
    "# Add prompt to creates plan on how to generate sql query\n",
    "grade_template = \"\"\"\n",
    "Give a binary score 'yes' or 'no' to indicate whether the context and the question needs sql query generation.\n",
    "Make sure to pay attention to the user input provided in the question.\n",
    "===\n",
    "Examples:\n",
    "Question: How can I reschedule my flight?\n",
    "Context: How to reschedule a flight: need to email returns@hpe.com, submit request, and someone will get back to you.\n",
    "binary_score=no\n",
    "\n",
    "Question: Can you tell me about my flight? my passenger id is \\\"3442 587242.\\\"\n",
    "Context: [Document( page_content='How to answer questions with What. Database has 11 rows: seat_no, fare_conditions, scheduled_arrival, scheduled_departure, ticket_no, book_ref, passenger_id, flight_id, flight_no, departure_airport, arrival_airport.\\n        The database is named query_results.'), Document( page_content='How to reschedule a flight: need to email returns@hpe.com, submit request, and someone will get back to you.'), Document( page_content='How to submit expense report: go to concur through home.hpe.com, and follow the necessary forms.')] \n",
    "binary_score=yes\n",
    "===\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_template(grade_template)\n",
    "\n",
    "rag_template = \"\"\"\n",
    "You are a helpful assistant that answers question based on retrieved context.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initalize Vector DB and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: uncomment if you want to use \n",
    "\n",
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "#     nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "#     assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "#     os.environ[\"NVIDIA_API_KEY\"] = nvapi_key\n",
    "# os.environ[\"NVIDIA_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG DB\n",
    "docs_list = [\n",
    "        {'metadata':{},'page_content':\"\"\"\n",
    "        How to answer questions with What. Database has 11 rows: seat_no, fare_conditions, scheduled_arrival, scheduled_departure, ticket_no, book_ref, passenger_id, flight_id, flight_no, departure_airport, arrival_airport.\n",
    "        The database is named query_results. \n",
    "        \"\"\"},\n",
    "        {'metadata':{},'page_content':'How to reschedule a flight: need to email returns@hpe.com, submit request, and someone will get back to you.'},\n",
    "        {'metadata':{},'page_content':'How to submit expense report: go to concur through home.hpe.com, and follow the necessary forms.'}\n",
    "    ]\n",
    "# NOTE: uncomment if you want to use NIM Embedding model\n",
    "# embeddings = NVIDIAEmbeddings(base_url='https://integrate.api.nvidia.com/v1',\n",
    "#                                 model=\"nvidia/nv-embed-v1\", \n",
    "#                                 api_key=os.environ[\"NVIDIA_API_KEY\"],\n",
    "#                                 truncate=\"NONE\")\n",
    "\n",
    "embeddings = NVIDIAEmbeddings(base_url='http://embedding-tyler.models.mlds-kserve.us.rdlabs.hpecorp.net',\n",
    "                                model=\"thenlper/gte-base\", \n",
    "                                api_key='',\n",
    "                                truncate=\"NONE\")\n",
    "\n",
    "\n",
    "retriever = create_retriever(docs_list,embeddings)\n",
    "local_db_path = 'tickets_joined.db'\n",
    "\n",
    "# Create models\n",
    "\n",
    "llm = ChatNVIDIA(base_url=\"http://10.182.1.167:8080/v1\",\n",
    "                  model=\"meta/llama-3.1-70b-instruct\", \n",
    "                   api_key=\"\\'\\'\",\n",
    "                   verbose=True)\n",
    "formatted_llm = ChatNVIDIA(base_url=\"http://10.182.1.167:8080/v1\",\n",
    "                  model=\"meta/llama-3.1-70b-instruct\", \n",
    "                   api_key=\"\\'\\'\",\n",
    "                   verbose=True).with_structured_output(FormatAns)\n",
    "plan_llm = ChatNVIDIA(base_url=\"http://10.182.1.167:8080/v1\",\n",
    "                  model=\"meta/llama-3.1-70b-instruct\", \n",
    "                   api_key=\"\\'\\'\",\n",
    "                   verbose=True).with_structured_output(Plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilites and code needed to run agent with LangGraph\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        structured_sql_query: Generated SQL query with user defined value\n",
    "        sql_results: data returned from SQL query (in list of json dicts)\n",
    "        plan: LLM generated plan how to generate the SQL query\n",
    "        documents: list of documents\n",
    "        times_transformed: number of times the question has been re-written\n",
    "        sql_gen_check: string of 'yes' or 'no' to determine if we should route to gen sql, or rag\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    structured_sql_query: FormatAns\n",
    "    sql_results: List[Dict]\n",
    "    plan: Plan\n",
    "    documents: List[str]\n",
    "    sql_gen_check: Grade\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    print(\"---RETRIEVE---\")\n",
    "    print(\"state: \",state)\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"documents\": documents, \"question\": question,'plan':None}\n",
    "\n",
    "\n",
    "def gen_sql_query(state):\n",
    "    print(\"---GENERATE_SQL_QUERY---\")\n",
    "    formatted = chat_prompt.invoke({\"context\": state['documents'], \n",
    "                                    \"question\": state['question']})\n",
    "    # print(formatted.to_string())\n",
    "    # generate answer\n",
    "    query_result = formatted_llm.invoke(formatted)\n",
    "    # print(\"query_result.user_input: \",query_result.user_input)\n",
    "    # print(\"query_result.user_input: \",query_result.sql_query)\n",
    "    return {\"documents\": state['documents'], \n",
    "            \"question\":  state['question'],\n",
    "            'plan': state['plan'],\n",
    "            \"structured_sql_query\": query_result}\n",
    "def execute_sql_query(state):\n",
    "    print(\"---EXEC_SQL_QUERY---\")\n",
    "    print(state)\n",
    "    query_result = state['structured_sql_query']\n",
    "    r = None\n",
    "    try:\n",
    "        r = call_db(local_db_path=local_db_path,\n",
    "                    query=query_result.sql_query,\n",
    "                    info=query_result.user_input)\n",
    "        print(\"r: \",r)\n",
    "        return {\"documents\": state['documents'], \n",
    "            \"question\":  state['question'],\n",
    "            \"structured_sql_query\": state['structured_sql_query'],\n",
    "            'plan': state['plan'],\n",
    "            \"sql_results\": r}\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return {\"documents\": state['documents'], \n",
    "            \"question\":  state['question'],\n",
    "            \"structured_sql_query\": state['structured_sql_query'],\n",
    "            'plan': state['plan'],\n",
    "            \"sql_results\": r}\n",
    "\n",
    "    \n",
    "def answer(state):\n",
    "    # By default, the new value returned by each node will override the prior state value.\n",
    "    print(\"---ANSWER---\")\n",
    "    print(\"state: \",state)\n",
    "\n",
    "    final_format = process_prompt.invoke({\"context\": state[\"sql_results\"], \"question\": state[\"question\"]})\n",
    "    print(final_format.to_string())\n",
    "    # structured_answer = parser.parse(answer)\n",
    "    answer = llm.invoke(final_format)\n",
    "    return {\"documents\": state[\"documents\"], \n",
    "            \"question\":  state[\"question\"], \n",
    "            \"structured_sql_query\": state['structured_sql_query'],\n",
    "            \"sql_results\": state['sql_results'],\n",
    "            'plan': state['plan'],\n",
    "            \"generation\":answer}\n",
    "def plan(state):\n",
    "    docs = state['documents']\n",
    "    question = state['question']\n",
    "    plan = plan_prompt.invoke({\"context\": docs, \"question\": question})\n",
    "    ans = plan_llm.invoke(plan)\n",
    "    return {'question':state['question'],\n",
    "            'documents':state['documents'],\n",
    "            'plan':ans}\n",
    "\n",
    "#update function\n",
    "def gen_sql_query2(state):\n",
    "    print(\"---GENERATE_SQL_QUERY---\")\n",
    "    plan = state['plan']\n",
    "    docs = state['documents']\n",
    "    question = state['question']\n",
    "    exec = execute_prompt.invoke({\"plan\": plan.plan, \"question\": question,'context':docs})\n",
    "    print(exec.to_string())\n",
    "    query_result = llm.with_structured_output(FormatAns).invoke(exec)\n",
    "    print(type(query_result))\n",
    "    # print(formatted.to_string())\n",
    "    # generate answer\n",
    "    # query_result = formatted_llm.invoke(ans2)\n",
    "    print(\"query_result.user_input: \",query_result.user_input)\n",
    "    print(\"query_result.user_input: \",query_result.sql_query)\n",
    "    return {\"documents\": state['documents'], \n",
    "            \"question\":  state['question'],\n",
    "            'plan': state['plan'],\n",
    "            \"structured_sql_query\": query_result}\n",
    "def check(state):\n",
    "    '''\n",
    "    '''\n",
    "    docs = state['documents']\n",
    "    print(\"docs: \",docs)\n",
    "    question = state['question']\n",
    "    print(\"question: \",question)\n",
    "    grade = grade_prompt.invoke({\"context\": docs, \"question\": question})\n",
    "    ans = llm.with_structured_output(Grade).invoke(grade)\n",
    "    print(\"ans: \",ans)\n",
    "    return {\"documents\": state['documents'], \"question\": state['question'],'sql_gen_check':ans,'plan':None}\n",
    "def decide(state):\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    ans = state['sql_gen_check'].binary_score\n",
    "    if ans == 'yes':\n",
    "        return 'yes'\n",
    "    elif ans == 'no':\n",
    "        return 'no'\n",
    "\n",
    "def rag_answer(state):\n",
    "    # By default, the new value returned by each node will override the prior state value.\n",
    "    print(\"---ANSWER---\")\n",
    "    print(\"state: \",state)\n",
    "\n",
    "    final_format = process_prompt.invoke({\"context\": state['documents'], \"question\": state[\"question\"]})\n",
    "    print(final_format.to_string())\n",
    "    # structured_answer = parser.parse(answer)\n",
    "    answer = llm.invoke(final_format)\n",
    "    return {\"documents\": state[\"documents\"], \n",
    "            \"question\":  state[\"question\"], \n",
    "            'plan': state['plan'],\n",
    "            \"generation\":answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Build graph\n",
    "builder3 = StateGraph(GraphState)\n",
    "builder3.add_node(\"retrieve\", retrieve)\n",
    "builder3.add_node(\"check\",check)\n",
    "builder3.add_node(\"generate_sql_query\",gen_sql_query2)\n",
    "builder3.add_node(\"execute_sql_query\",execute_sql_query)\n",
    "builder3.add_node(\"planning\",plan)\n",
    "builder3.add_node(\"answer\", answer)\n",
    "builder3.add_node(\"rag_answer\", rag_answer)\n",
    "# Build Edges\n",
    "builder3.add_edge(START, \"retrieve\")\n",
    "builder3.add_edge('retrieve','check')\n",
    "builder3.add_conditional_edges('check',\n",
    "                               decide,\n",
    "                               {\n",
    "                                   'yes':'planning',\n",
    "                                   'no': 'rag_answer'},\n",
    "                            )\n",
    "# builder3.add_edge(\"retrieve\", \"planning\")\n",
    "builder3.add_edge(\"planning\",'generate_sql_query')\n",
    "builder3.add_edge(\"generate_sql_query\",\"execute_sql_query\")\n",
    "builder3.add_edge(\"execute_sql_query\",\"answer\")\n",
    "builder3.add_edge(\"answer\", END)\n",
    "# Compile\n",
    "app3 = builder3.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compile the graph\n",
    "# # app = builder3.compile()\n",
    "# from IPython.display import Image, display\n",
    "\n",
    "# display(Image(app3.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run graph\n",
    "inputs = {\"question\": \"Can you tell me about my flight's departure time? my ticket_no is \\\"7240005432906569\\\".\"}\n",
    "for output in app3.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        print(f\"Node '{key}':\")\n",
    "# Final generation\n",
    "print(value[\"generation\"].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run graph\n",
    "inputs = {\"question\": \"How can I reschedule my flight?\"}\n",
    "for output in app3.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        print(f\"Node '{key}':\")\n",
    "# Final generation\n",
    "print(value[\"generation\"].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nim-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
