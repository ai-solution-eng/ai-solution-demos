{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3051ba77-9d36-477c-9969-b07c4b776b6f",
   "metadata": {},
   "source": [
    "# Part III: Model Evaluation Using NeMo Evaluator\n",
    "\n",
    "This notebook covers the following:\n",
    "\n",
    "0. [Pre-requisites: Configurations and Health Checks](#step-0)\n",
    "1. [Establish a baseline accuracy benchmark](#step-1). This uses the off-the-shelf llama-3.2-1b-instruct model\n",
    "2. [Evaluate the LoRA customized model](#step-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5adb3981-0171-4080-96e8-245954b63dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep, time\n",
    "from nemo_microservices import NeMoMicroservices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb4848b-8bdf-4ebf-9f82-2c2cd61cb097",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"step-0\"></a>\n",
    "## Prerequisites: Configurations and Health Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ac797d-ea7b-497e-a6c4-357a2ca3367d",
   "metadata": {},
   "source": [
    "Before you proceed, make sure that you completed the previous notebooks on data preparation and model fine-tuning to obtain the assets required to follow along."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23824d49-22a8-4480-ba49-9745a0069f86",
   "metadata": {},
   "source": [
    "### Configure NeMo Microservices Endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd69e09",
   "metadata": {},
   "source": [
    "The following code imports necessary configurations and prints the endpoints for the NeMo Data Store, Entity Store, Customizer, Evaluator, and NIM, as well as the namespace and base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf007c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "\n",
    "# # Initialize NeMo Microservices SDK client\n",
    "# nemo_client = NeMoMicroservices(\n",
    "#     base_url=NEMO_URL,\n",
    "#     inference_base_url=NIM_URL,\n",
    "# )\n",
    "\n",
    "# Separate SDK clients per service\n",
    "entity_client = NeMoMicroservices(base_url=NEMO_ENTITY_STORE_URL, inference_base_url=NIM_URL)\n",
    "customizer_client = NeMoMicroservices(base_url=NEMO_CUSTOMIZER_URL, inference_base_url=NIM_URL)\n",
    "evaluator_client = NeMoMicroservices(base_url=NEMO_EVALUATOR_URL, inference_base_url=NIM_URL)\n",
    "guardrails_client = NeMoMicroservices(base_url=NEMO_GUARDRAILS_URL, inference_base_url=NIM_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a5f5407-5495-42b7-b472-e1d859e63421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Store endpoint: http://nemo-data-store.nemo.svc.cluster.local:3000\n",
      "Entity Store endpoint: http://nemo-entity-store.nemo.svc.cluster.local:8000\n",
      "Customizer endpoint: http://nemo-customizer.nemo.svc.cluster.local:8000\n",
      "Evaluator endpoint: http://nemo-evaluator.nemo.svc.cluster.local:7331\n",
      "NIM endpoint: http://nemo-nim-proxy.nemo.svc.cluster.local:8000\n",
      "Namespace: xlam-tutorial-ns\n",
      "Base Model for Customization: meta/llama-3.2-1b-instruct@v1.0.0+L40\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data Store endpoint: {NEMO_DATA_STORE_URL}\")\n",
    "print(f\"Entity Store endpoint: {NEMO_ENTITY_STORE_URL}\")\n",
    "print(f\"Customizer endpoint: {NEMO_CUSTOMIZER_URL}\")\n",
    "print(f\"Evaluator endpoint: {NEMO_EVALUATOR_URL}\")\n",
    "print(f\"NIM endpoint: {NIM_URL}\")\n",
    "print(f\"Namespace: {NMS_NAMESPACE}\")\n",
    "print(f\"Base Model for Customization: {BASE_MODEL}@{BASE_MODEL_VERSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c6449-453b-4372-81fe-7baa9aea1b1a",
   "metadata": {},
   "source": [
    "### Check Available Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6fe718",
   "metadata": {},
   "source": [
    "Specify the customized model name that you got from the previous notebook to the following variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07faee8b-cc39-4485-abf4-8ab884883d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOMIZED_MODEL = \"xlam-tutorial-ns/llama-3.2-1b-xlam-run1@v1\" # CUSTOM_MODEL # paste from the previous notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473c0ea9",
   "metadata": {},
   "source": [
    "The following code checks if the NIM endpoint hosts the model properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1b6072e-66a1-43f6-a19e-d506ad95281b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET http://nemo-nim-proxy.nemo.svc.cluster.local:8000/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Check if the custom LoRA model is hosted by NVIDIA NIM\n",
    "models = entity_client.inference.models.list()\n",
    "model_names = [model.id for model in models.data]\n",
    "\n",
    "assert CUSTOMIZED_MODEL in model_names, \\\n",
    "    f\"Model {CUSTOMIZED_MODEL} not found\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35403c52-0448-49b4-aaf8-d80cf2a05226",
   "metadata": {},
   "source": [
    "### Verify the Availability of the Datasets\n",
    "\n",
    "In the previous notebook, we uploaded the test dataset along with the train and validation sets using `nemo_client.datasets.create(name=DATASET_NAME, namespace=NMS_NAMESPACE, ...)`.\n",
    "The following code performs a sanity check to validate the dataset by retrieving it and printing the URL of the files in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2267b33-43da-4623-832a-65ef9f6c2525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET http://nemo-entity-store.nemo.svc.cluster.local:8000/v1/datasets/xlam-tutorial-ns/xlam-ft-dataset \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files URL: hf://datasets/xlam-tutorial-ns/xlam-ft-dataset\n"
     ]
    }
   ],
   "source": [
    "# Sanity check to validate dataset\n",
    "dataset = entity_client.datasets.retrieve(namespace=NMS_NAMESPACE, dataset_name=DATASET_NAME)\n",
    "print(\"Files URL:\", dataset.files_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162f9282-abec-401d-9143-076a3977d9a9",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"step-1\"></a>\n",
    "## Step 1: Establish Baseline Accuracy Benchmark\n",
    "\n",
    "First, we’ll assess the accuracy of the 'off-the-shelf' base model—pristine, untouched, and blissfully unaware of the transformative magic that is fine-tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147cf266-5e90-4b81-92a5-53b7981c47be",
   "metadata": {},
   "source": [
    "### 1.1: Create an Evaluation Config Object\n",
    "Create an evaluation configuration object for NeMo Evaluator. For more information on various parameters, refer to the [NeMo Evaluator configuration](https://docs.nvidia.com/nemo/microservices/latest/evaluate/evaluation-configs.html) in the NeMo microservices documentation.\n",
    "\n",
    "\n",
    "* The `tasks.custom-tool-calling.dataset.files_url` is used to indicate which test file to use. Note that it's required to upload this to the NeMo Data Store and register with Entity store before using.\n",
    "* The `tasks.dataset.limit` argument below specifies how big a subset of test data to run the evaluation on\n",
    "* The evaluation metric `tasks.metrics.tool-calling-accuracy` reports `function_name_accuracy` and `function_name_and_args_accuracy` numbers, which are as their names imply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84e2e116-ed60-4e88-970b-f0d09d4a258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tool_calling_eval_config = {\n",
    "    \"type\": \"custom\",\n",
    "    \"tasks\": {\n",
    "        \"custom-tool-calling\": {\n",
    "            \"type\": \"chat-completion\",\n",
    "            \"dataset\": {\n",
    "                \"files_url\": f\"hf://datasets/{NMS_NAMESPACE}/{DATASET_NAME}/testing/xlam-test-single.jsonl\",\n",
    "                \"limit\": 50\n",
    "            },\n",
    "            \"params\": {\n",
    "                \"template\": {\n",
    "                    \"messages\": \"{{ item.messages | tojson}}\",\n",
    "                    \"tools\": \"{{ item.tools | tojson }}\",\n",
    "                    \"tool_choice\": \"auto\"\n",
    "                }\n",
    "            },\n",
    "            \"metrics\": {\n",
    "                \"tool-calling-accuracy\": {\n",
    "                    \"type\": \"tool-calling\",\n",
    "                    \"params\": {\"tool_calls_ground_truth\": \"{{ item.tool_calls | tojson }}\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8007f17-2eb1-477f-b156-5ed1d17d894b",
   "metadata": {},
   "source": [
    "### 1.2: Launch Evaluation Job "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5fbf88",
   "metadata": {},
   "source": [
    "The following code calls the `nemo_client.evaluation.jobs.create()` method to launch an evaluation job in the NeMo Evaluator.\n",
    "It uses the evaluation configuration defined in the previous cell and targets the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69c5ec90-a189-47d6-9c07-18162b95130d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://nemo-evaluator.nemo.svc.cluster.local:7331/v1/evaluation/jobs \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created evaluation job: eval-6EiFmVb4ed3X3TCKKXKSBd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvaluationJob(config=EvaluationConfig(type='custom', id='eval-config-4UBwcuem1iswexdGBcxkcp', created_at=datetime.datetime(2025, 8, 25, 3, 47, 17, 244169), custom_fields={}, description=None, groups=None, name='eval-config-4UBwcuem1iswexdGBcxkcp', namespace='default', ownership=None, params=None, project=None, schema_version='1.0', tasks={'custom-tool-calling': TaskConfig(type='chat-completion', dataset=DatasetEv(files_url='hf://datasets/xlam-tutorial-ns/xlam-ft-dataset/testing/xlam-test-single.jsonl', id='dataset-Gn47mZTKb6EWSy5i5nV1WH', created_at=datetime.datetime(2025, 8, 25, 3, 47, 17, 244344), custom_fields={}, description=None, format=None, hf_endpoint=None, limit=50, name='dataset-Gn47mZTKb6EWSy5i5nV1WH', namespace='default', ownership=None, project=None, schema_version='1.0', split=None, type_prefix=None, updated_at=datetime.datetime(2025, 8, 25, 3, 47, 17, 244345), version_id='main', version_tags=[]), metrics={'tool-calling-accuracy': MetricConfig(type='tool-calling', params={'tool_calls_ground_truth': '{{ item.tool_calls | tojson }}'})}, params={'template': {'messages': '{{ item.messages | tojson}}', 'tools': '{{ item.tools | tojson }}', 'tool_choice': 'auto'}})}, type_prefix='eval-config', updated_at=datetime.datetime(2025, 8, 25, 3, 47, 17, 244170)), target=EvaluationTarget(type='model', id='eval-target-73NWbaBd1CP4nGCG6CWLEC', cached_outputs=None, created_at=datetime.datetime(2025, 8, 25, 3, 47, 17, 244418), custom_fields={}, dataset=None, description=None, model='meta/llama-3.2-1b-instruct', name='eval-target-73NWbaBd1CP4nGCG6CWLEC', namespace='default', ownership=None, project=None, rag=None, retriever=None, rows=None, schema_version='1.0', type_prefix='eval-target', updated_at=datetime.datetime(2025, 8, 25, 3, 47, 17, 244418)), id='eval-6EiFmVb4ed3X3TCKKXKSBd', created_at=datetime.datetime(2025, 8, 25, 3, 47, 17, 244774), custom_fields={}, description=None, namespace='default', output_files_url=None, ownership=None, project=None, result=None, status='created', status_details=EvaluationStatusDetails(message=None, progress=None, samples_processed=None, task_status={}), updated_at=datetime.datetime(2025, 8, 25, 3, 47, 17, 244774))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create evaluation job for the base model\n",
    "eval_job = evaluator_client.evaluation.jobs.create(\n",
    "    config=simple_tool_calling_eval_config,\n",
    "    target={\"type\": \"model\", \"model\": BASE_MODEL}\n",
    ")\n",
    "\n",
    "base_eval_job_id = eval_job.id\n",
    "print(f\"Created evaluation job: {base_eval_job_id}\")\n",
    "eval_job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc5133e-076c-4560-815a-1ff71901af01",
   "metadata": {},
   "source": [
    "The following code defines a helper function to poll on job status until it finishes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fa0b694-a699-4eb0-903c-9bac8651f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_eval_job(nemo_client, job_id: str, polling_interval: int = 10, timeout: int = 6000):\n",
    "    \"\"\"Helper for waiting an eval job.\"\"\"\n",
    "    start_time = time()\n",
    "    job = nemo_client.evaluation.jobs.retrieve(job_id=job_id)\n",
    "    status = job.status\n",
    "\n",
    "    while (status in [\"pending\", \"created\", \"running\"]):\n",
    "        # Check for timeout\n",
    "        if time() - start_time > timeout:\n",
    "            raise RuntimeError(f\"Took more than {timeout} seconds.\")\n",
    "\n",
    "        # Sleep before polling again\n",
    "        sleep(polling_interval)\n",
    "\n",
    "        # Fetch updated status and progress\n",
    "        job = nemo_client.evaluation.jobs.retrieve(job_id=job_id)\n",
    "        status = job.status\n",
    "\n",
    "        # Progress details (only fetch if status is \"running\")\n",
    "        progress = 0\n",
    "        if status == \"running\" and job.status_details:\n",
    "            progress = job.status_details.progress or 0\n",
    "        elif status == \"completed\":\n",
    "            progress = 100\n",
    "\n",
    "        print(f\"Job status: {status} after {time() - start_time:.2f} seconds. Progress: {progress}%\")\n",
    "\n",
    "    return job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49db71fc",
   "metadata": {},
   "source": [
    "Run the helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48322fdb-8920-435d-863d-9a75158843d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET http://nemo-evaluator.nemo.svc.cluster.local:7331/v1/evaluation/jobs/eval-6EiFmVb4ed3X3TCKKXKSBd \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET http://nemo-evaluator.nemo.svc.cluster.local:7331/v1/evaluation/jobs/eval-6EiFmVb4ed3X3TCKKXKSBd \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: running after 5.02 seconds. Progress: 94.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET http://nemo-evaluator.nemo.svc.cluster.local:7331/v1/evaluation/jobs/eval-6EiFmVb4ed3X3TCKKXKSBd \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: running after 10.03 seconds. Progress: 96.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET http://nemo-evaluator.nemo.svc.cluster.local:7331/v1/evaluation/jobs/eval-6EiFmVb4ed3X3TCKKXKSBd \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: running after 15.04 seconds. Progress: 96.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET http://nemo-evaluator.nemo.svc.cluster.local:7331/v1/evaluation/jobs/eval-6EiFmVb4ed3X3TCKKXKSBd \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: running after 20.04 seconds. Progress: 98.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET http://nemo-evaluator.nemo.svc.cluster.local:7331/v1/evaluation/jobs/eval-6EiFmVb4ed3X3TCKKXKSBd \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: running after 25.05 seconds. Progress: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET http://nemo-evaluator.nemo.svc.cluster.local:7331/v1/evaluation/jobs/eval-6EiFmVb4ed3X3TCKKXKSBd \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: completed after 30.06 seconds. Progress: 100%\n"
     ]
    }
   ],
   "source": [
    "# Poll\n",
    "job = wait_eval_job(evaluator_client, base_eval_job_id, polling_interval=5, timeout=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de58a76-1775-4c93-8798-1ec89c8eeaff",
   "metadata": {},
   "source": [
    "### 1.3 Review Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bb6220",
   "metadata": {},
   "source": [
    "The following code retrieves the evaluation results for the base evaluation job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec04c15a-62a9-4271-8a7f-9a7e8ec7884d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET http://nemo-evaluator.nemo.svc.cluster.local:7331/v1/evaluation/jobs/eval-6EiFmVb4ed3X3TCKKXKSBd/results \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"job\": \"eval-6EiFmVb4ed3X3TCKKXKSBd\",\n",
      "  \"id\": \"evaluation_result-XXLd1qjUdSTRd641fi5yKG\",\n",
      "  \"created_at\": \"2025-08-25T03:47:17.425116\",\n",
      "  \"custom_fields\": {},\n",
      "  \"files_url\": \"hf://datasets/evaluation-results/eval-6EiFmVb4ed3X3TCKKXKSBd\",\n",
      "  \"groups\": {},\n",
      "  \"namespace\": \"default\",\n",
      "  \"tasks\": {\n",
      "    \"custom-tool-calling\": {\n",
      "      \"metrics\": {\n",
      "        \"tool-calling-accuracy\": {\n",
      "          \"scores\": {\n",
      "            \"function_name_accuracy\": {\n",
      "              \"value\": 0.34,\n",
      "              \"stats\": {\n",
      "                \"count\": 50,\n",
      "                \"mean\": 0.34,\n",
      "                \"sum\": 17.0\n",
      "              }\n",
      "            },\n",
      "            \"function_name_and_args_accuracy\": {\n",
      "              \"value\": 0.2,\n",
      "              \"stats\": {\n",
      "                \"count\": 50,\n",
      "                \"mean\": 0.2,\n",
      "                \"sum\": 10.0\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"updated_at\": \"2025-08-25T04:01:22.037924\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "results = evaluator_client.evaluation.jobs.results(job_id=base_eval_job_id)\n",
    "print(results.model_dump_json(indent=2, exclude_unset=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b28813",
   "metadata": {},
   "source": [
    "The following code extracts and prints the accuracy scores for the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2129c739-8980-4d28-ab72-331e00f0c2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model: function_name_accuracy: 0.34\n",
      "Base model: function_name_and_args_accuracy: 0.2\n"
     ]
    }
   ],
   "source": [
    "# Extract function name accuracy score\n",
    "base_scores = results.tasks[\"custom-tool-calling\"].metrics[\"tool-calling-accuracy\"].scores\n",
    "base_function_name_accuracy_score = base_scores[\"function_name_accuracy\"].value\n",
    "base_function_name_and_args_accuracy = base_scores[\"function_name_and_args_accuracy\"].value\n",
    "\n",
    "print(f\"Base model: function_name_accuracy: {base_function_name_accuracy_score}\")\n",
    "print(f\"Base model: function_name_and_args_accuracy: {base_function_name_and_args_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9fcb78-0e23-45f6-988a-8be6e33c2cfc",
   "metadata": {},
   "source": [
    "Without any finetuning, the `meta/llama-3.2-1b-instruct` model should score in the ballpark of about 12% in `function_name_accuracy`, and 8% in `function_name_and_args_accuracy` (note that scores will vary by about +/-4% due to non-determinism of LLMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c7ab4-bb3c-46c3-a51e-9e25509a7781",
   "metadata": {},
   "source": [
    "### (Optional) 1.4 Download and Inspect Results\n",
    "\n",
    "To take a deeper look into the model's generated outputs, you can download and review the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bf02e0-2383-46d5-ad47-3edc1e1dfdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_evaluation_results(nemo_client, eval_job_id, output_file):\n",
    "    \"\"\"Downloads evaluation results for a given job ID.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Get download results\n",
    "        results = nemo_client.evaluation.jobs.download_results(job_id=eval_job_id)\n",
    "        \n",
    "        # Save the results to a file\n",
    "        results.write_to_file(output_file)\n",
    "\n",
    "        print(f\"Evaluation results for job {eval_job_id} downloaded successfully to {output_file}.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download evaluation results: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8ec595-8edd-4d8c-a2a0-29f830710c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = f\"{base_eval_job_id}.zip\"\n",
    "\n",
    "# Assertion fails if download fails\n",
    "assert download_evaluation_results(nemo_client=nemo_client, eval_job_id=base_eval_job_id, output_file=output_file) == True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687f633d-5b20-4a28-8962-970a54b91f40",
   "metadata": {},
   "source": [
    "You can inspect the downloaded results file to observe places where the base model errors. Without any fine-tuning, some models not only return inaccurate function names and arguments, but they may not adhere to a consistent structured / predictable output schema. This makes it difficult to automatically parse these outputs, deterring integration with external systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79229ce6-4ce4-4f82-b0c9-15be351a2291",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"step-2\"></a>\n",
    "## Step 2: Evaluate the LoRA Customized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdeba8-6800-419f-b5c5-0dc34bf6e0f5",
   "metadata": {},
   "source": [
    "### 2.1 Launch Evaluation Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb900e7-2ec8-417a-9266-2ce682f655e5",
   "metadata": {},
   "source": [
    "Run another evaluation job with the same evaluation config but with the customized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5438608-e708-4421-bf45-da39fbe506ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://nemo-evaluator.nemo.svc.cluster.local:7331/v1/evaluation/jobs \"HTTP/1.1 201 Created\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created evaluation job for customized model: eval-RFZY5WxsHQ4S6BMQmYb2aA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvaluationJob(config=EvaluationConfig(type='custom', id='eval-config-C9gggwDWfUypZdtofEbwAE', created_at=datetime.datetime(2025, 8, 25, 4, 2, 29, 683769), custom_fields={}, description=None, groups=None, name='eval-config-C9gggwDWfUypZdtofEbwAE', namespace='default', ownership=None, params=None, project=None, schema_version='1.0', tasks={'custom-tool-calling': TaskConfig(type='chat-completion', dataset=DatasetEv(files_url='hf://datasets/xlam-tutorial-ns/xlam-ft-dataset/testing/xlam-test-single.jsonl', id='dataset-3b5YFqDrvUeRBQXurUjUDV', created_at=datetime.datetime(2025, 8, 25, 4, 2, 29, 683838), custom_fields={}, description=None, format=None, hf_endpoint=None, limit=50, name='dataset-3b5YFqDrvUeRBQXurUjUDV', namespace='default', ownership=None, project=None, schema_version='1.0', split=None, type_prefix=None, updated_at=datetime.datetime(2025, 8, 25, 4, 2, 29, 683838), version_id='main', version_tags=[]), metrics={'tool-calling-accuracy': MetricConfig(type='tool-calling', params={'tool_calls_ground_truth': '{{ item.tool_calls | tojson }}'})}, params={'template': {'messages': '{{ item.messages | tojson}}', 'tools': '{{ item.tools | tojson }}', 'tool_choice': 'auto'}})}, type_prefix='eval-config', updated_at=datetime.datetime(2025, 8, 25, 4, 2, 29, 683771)), target=EvaluationTarget(type='model', id='eval-target-9cGQNJxpG6GupM51wKk1gh', cached_outputs=None, created_at=datetime.datetime(2025, 8, 25, 4, 2, 29, 683889), custom_fields={}, dataset=None, description=None, model='xlam-tutorial-ns/llama-3.2-1b-xlam-run1@v1', name='eval-target-9cGQNJxpG6GupM51wKk1gh', namespace='default', ownership=None, project=None, rag=None, retriever=None, rows=None, schema_version='1.0', type_prefix='eval-target', updated_at=datetime.datetime(2025, 8, 25, 4, 2, 29, 683889)), id='eval-RFZY5WxsHQ4S6BMQmYb2aA', created_at=datetime.datetime(2025, 8, 25, 4, 2, 29, 684143), custom_fields={}, description=None, namespace='default', output_files_url=None, ownership=None, project=None, result=None, status='created', status_details=EvaluationStatusDetails(message=None, progress=None, samples_processed=None, task_status={}), updated_at=datetime.datetime(2025, 8, 25, 4, 2, 29, 684144))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create evaluation job for customized model\n",
    "ft_eval_job = evaluator_client.evaluation.jobs.create(\n",
    "    config=simple_tool_calling_eval_config,\n",
    "    target={\"type\": \"model\", \"model\": CUSTOMIZED_MODEL}\n",
    ")\n",
    "\n",
    "ft_eval_job_id = ft_eval_job.id\n",
    "print(f\"Created evaluation job for customized model: {ft_eval_job_id}\")\n",
    "ft_eval_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7409f25-23c1-4574-9392-051f3da0498c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET http://nemo-evaluator.nemo.svc.cluster.local:7331/v1/evaluation/jobs/eval-RFZY5WxsHQ4S6BMQmYb2aA \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET http://nemo-evaluator.nemo.svc.cluster.local:7331/v1/evaluation/jobs/eval-RFZY5WxsHQ4S6BMQmYb2aA \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: running after 5.02 seconds. Progress: 52.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET http://nemo-evaluator.nemo.svc.cluster.local:7331/v1/evaluation/jobs/eval-RFZY5WxsHQ4S6BMQmYb2aA \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: running after 10.03 seconds. Progress: 62.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET http://nemo-evaluator.nemo.svc.cluster.local:7331/v1/evaluation/jobs/eval-RFZY5WxsHQ4S6BMQmYb2aA \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: running after 15.03 seconds. Progress: 76.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET http://nemo-evaluator.nemo.svc.cluster.local:7331/v1/evaluation/jobs/eval-RFZY5WxsHQ4S6BMQmYb2aA \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: running after 20.11 seconds. Progress: 92.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET http://nemo-evaluator.nemo.svc.cluster.local:7331/v1/evaluation/jobs/eval-RFZY5WxsHQ4S6BMQmYb2aA \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: running after 25.11 seconds. Progress: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET http://nemo-evaluator.nemo.svc.cluster.local:7331/v1/evaluation/jobs/eval-RFZY5WxsHQ4S6BMQmYb2aA \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: completed after 30.12 seconds. Progress: 100%\n"
     ]
    }
   ],
   "source": [
    "# Poll\n",
    "job = wait_eval_job(evaluator_client, ft_eval_job_id, polling_interval=5, timeout=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3706c060-8b37-40e2-b0e6-636d130260e7",
   "metadata": {},
   "source": [
    "### 2.2 Review Evaluation Metrics\n",
    "The following code retrieves the evaluation results for the fine-tuned model evaluation job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d2ffdb8-8e8e-403e-a836-8bcd93778814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET http://nemo-evaluator.nemo.svc.cluster.local:7331/v1/evaluation/jobs/eval-RFZY5WxsHQ4S6BMQmYb2aA/results \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"job\": \"eval-RFZY5WxsHQ4S6BMQmYb2aA\",\n",
      "  \"id\": \"evaluation_result-DWuRg5NLvrW5QxPapespoY\",\n",
      "  \"created_at\": \"2025-08-25T04:02:29.847772\",\n",
      "  \"custom_fields\": {},\n",
      "  \"description\": null,\n",
      "  \"files_url\": \"hf://datasets/evaluation-results/eval-RFZY5WxsHQ4S6BMQmYb2aA\",\n",
      "  \"groups\": {},\n",
      "  \"namespace\": \"default\",\n",
      "  \"ownership\": null,\n",
      "  \"project\": null,\n",
      "  \"tasks\": {\n",
      "    \"custom-tool-calling\": {\n",
      "      \"metrics\": {\n",
      "        \"tool-calling-accuracy\": {\n",
      "          \"scores\": {\n",
      "            \"function_name_accuracy\": {\n",
      "              \"value\": 0.98,\n",
      "              \"stats\": {\n",
      "                \"count\": 50,\n",
      "                \"max\": null,\n",
      "                \"mean\": 0.98,\n",
      "                \"min\": null,\n",
      "                \"stddev\": null,\n",
      "                \"stderr\": null,\n",
      "                \"sum\": 49.0,\n",
      "                \"sum_squared\": null,\n",
      "                \"variance\": null\n",
      "              }\n",
      "            },\n",
      "            \"function_name_and_args_accuracy\": {\n",
      "              \"value\": 0.7,\n",
      "              \"stats\": {\n",
      "                \"count\": 50,\n",
      "                \"max\": null,\n",
      "                \"mean\": 0.7,\n",
      "                \"min\": null,\n",
      "                \"stddev\": null,\n",
      "                \"stderr\": null,\n",
      "                \"sum\": 35.0,\n",
      "                \"sum_squared\": null,\n",
      "                \"variance\": null\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"updated_at\": \"2025-08-25T04:03:08.776937\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Get evaluation results for customized model\n",
    "ft_results = evaluator_client.evaluation.jobs.results(job_id=ft_eval_job_id)\n",
    "print(ft_results.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adb03140-21ad-445b-b173-7aebdfb0b703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom model: function_name_accuracy: 0.98\n",
      "Custom model: function_name_and_args_accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "# Extract function name accuracy score\n",
    "ft_scores = ft_results.tasks[\"custom-tool-calling\"].metrics[\"tool-calling-accuracy\"].scores\n",
    "ft_function_name_accuracy_score = ft_scores[\"function_name_accuracy\"].value\n",
    "ft_function_name_and_args_accuracy = ft_scores[\"function_name_and_args_accuracy\"].value\n",
    "\n",
    "print(f\"Custom model: function_name_accuracy: {ft_function_name_accuracy_score}\")\n",
    "print(f\"Custom model: function_name_and_args_accuracy: {ft_function_name_and_args_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f568a204-ad01-4a04-8cfa-602816b8937c",
   "metadata": {},
   "source": [
    "A successfully fine-tuned `meta/llama-3.2-1b-instruct` results in a significant increase in tool calling accuracy with \n",
    "\n",
    "In this case you should observe roughly the following improvements -\n",
    "* function_name_accuracy: 12% to 92%\n",
    "* function_name_and_args_accuracy: 8% to 72%\n",
    "\n",
    "Since this evaluation was on a limited number of samples for demonstration purposes, you may choose to increase `tasks.dataset.limit` in your evaluation config `simple_tool_calling_eval_config`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcc53b6-677b-4b44-bf6e-bcdadef21a73",
   "metadata": {},
   "source": [
    "## (Optional) Next Steps\n",
    "\n",
    "\n",
    "\n",
    "* You may also run the same evaluation on a base `meta/llama-3.1-70b-instruct` model for comparison.\n",
    "For this, first you will need to deploy the corresponding NIM using instructions [here](https://build.nvidia.com/meta/llama-3_1-70b-instruct/deploy). After your NIM is deployed, set that endpoint as your evaluation target like so -\n",
    "\n",
    "``` python\n",
    "# Create an evaluation target\n",
    "NIM_URL = \"http://0.0.0.0:8000\"\n",
    "EVAL_TARGET = {\n",
    "    \"type\": \"model\", \n",
    "    \"model\": {\n",
    "       \"api_endpoint\": {\n",
    "         \"url\": f\"{NIM_URL}/v1/completions\",\n",
    "         \"model_id\": \"meta/llama-3.1-70b-instruct\",\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Start eval job\n",
    "eval_job = nemo_client.evaluation.jobs.create(\n",
    "    config=simple_tool_calling_eval_config,\n",
    "    target=EVAL_TARGET\n",
    ")\n",
    "```\n",
    "\n",
    "Running evaluation using the default config in this notebook, you should observe `meta/llama-3.1-70b-instruct` performance similar to -\n",
    "* function_name_accuracy: 98%\n",
    "* function_name_and_args_accuracy: 66%\n",
    "\n",
    "Remarkably, a LoRA-tuned `meta/llama-3.2-1b-instruct` achieves accuracy that is close to a model 70 times its size, even outperforming it in the combined `function_name_and_args_accuracy` score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a406f95b-4fc5-462a-ad6c-43196b70896d",
   "metadata": {},
   "source": [
    "You can now proceed with the same processes to fine-tune other NIM for LLMs and evaluate the accuracies between the base model and the fine-tuned model. By doing so, you can produce more accurate models for your use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
